{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffc37db9-080c-413c-9499-ef151074ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b226ebff-8131-4b43-96a6-a6ecf33a8294",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = \" MetricsDB has 3 main components. Cluster Manager is responsible for assigning partitions to backend servers. HDFS is used to store mappings from partitions to servers. Backend Servers are responsible for processing metrics for a small number of partitions. Each Backend Server keeps the latest two hours of data for all metrics in memory and they checkpoint in-memory data every two hours to durable storage, Blobstore.  Coordinators are responsible for routing requests to all replica sets and validates desired quorum constraints are met.  MetricsDB is multi-zone compliant. MetricsDB has a replication factor of three. Apache Kafka producer has an option to batch requests, which helped us reduce the number of requests to the queue and storage.\"\n",
    "#Resp 03 and 10:\n",
    "doc_2 = \"The diagram describes a system architecture in a simple overview. The entity cordinator access the Kafka platform as the the replicas sets do too. Each replica set is composed by two elements: the cluster manager that access the HDFS dabase and the Backend Servers that access the Blobstore database. \"\n",
    "doc_3 = \"coordinator use kafka message service that has to distribute the transactions to hadoop and blob storage service\"\n",
    "\n",
    "doc_4 = \"Whenever an event occurs in the Spotify client, it is sent to one of the Spotify gateways which logs it via syslog. There it is assigned a timestamp that is used throughout the event delivery system.  all data needs to be delivered to a centrally located Hadoop cluster. All data that gets delivered via our event delivery service is written on HDFS. The File Tailer tails log files looking for new events, and forwards them to the Event Delivery Service. As soon as it gets a confirmation that the event has been received it’s responsibility ends. The Event Delivery Service accepts events from the Tailer, transform them to their final structured format and forwards them to the Kafka Brokers. It is built as a RESTful microservice using the Apollo framework and deployed using the Helios orchestration platform, a common design pattern at Spotify. Embedding a simple Kafka producer in the Event Delivery Service also proved to be easy. The Mirror Maker project introduced mirroring between data centers, and the Camus project can be used for exporting Avro structured events to hourly buckets.\"\n",
    "doc_5 = \"DBLog is a Generic Change-Data-Capture Framework that allows capturing committed changes from a database in real-time and propagating those changes to downstream consumers. Transaction logs from databases are the source of CDC events. DBLog is a Java-based framework, able to capture changes in real-time and to take dumps. We use Zookeeper to store state related to log and dump processing, and for leader election. In order to use DBLog a database needs to provide a change log from a linear history of committed changes and these conditions are fulfilled by systems like MySQL, PostgreSQL, MariaDB, etc. Dump processing was integrated by using SQL and JDBC, only requiring to implement the chunk selection and watermark update. The same code is used for MySQL and PostgreSQL and can be used for other similar databases as well. DBLog uses an active-passive architecture. One instance is active and the others are passive standbys. We leverage Zookeeper for leader election to determine the active instance.\"\n",
    "\n",
    "corpus = np.array([doc_1, doc_2, doc_3, doc_4, doc_5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f35b20-18bb-4563-a75e-ff712445b5cb",
   "metadata": {},
   "source": [
    "### 1) Aplicar lowercase e remover caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d708b90-8a3b-4820-82c5-2f3dcfc4b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    pattern = \"[{}]\".format(string.punctuation)\n",
    "    text = [word.lower() for word in text]\n",
    "    text = [[re.sub(pattern, \"\", word) for word in words.split()] for words in text]\n",
    "    text = [[word for word in words if len(word)>1] for words in text]    \n",
    "    text = [' '.join(words) for words in text]\n",
    "    return np.array(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec2a0c77-c5a5-4514-af39-ec8272be8c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['metricsdb has main components cluster manager is responsible for assigning partitions to backend servers hdfs is used to store mappings from partitions to servers backend servers are responsible for processing metrics for small number of partitions each backend server keeps the latest two hours of data for all metrics in memory and they checkpoint inmemory data every two hours to durable storage blobstore coordinators are responsible for routing requests to all replica sets and validates desired quorum constraints are met metricsdb is multizone compliant metricsdb has replication factor of three apache kafka producer has an option to batch requests which helped us reduce the number of requests to the queue and storage'\n",
      " 'the diagram describes system architecture in simple overview the entity cordinator access the kafka platform as the the replicas sets do too each replica set is composed by two elements the cluster manager that access the hdfs dabase and the backend servers that access the blobstore database'\n",
      " 'coordinator use kafka message service that has to distribute the transactions to hadoop and blob storage service'\n",
      " 'whenever an event occurs in the spotify client it is sent to one of the spotify gateways which logs it via syslog there it is assigned timestamp that is used throughout the event delivery system all data needs to be delivered to centrally located hadoop cluster all data that gets delivered via our event delivery service is written on hdfs the file tailer tails log files looking for new events and forwards them to the event delivery service as soon as it gets confirmation that the event has been received it’s responsibility ends the event delivery service accepts events from the tailer transform them to their final structured format and forwards them to the kafka brokers it is built as restful microservice using the apollo framework and deployed using the helios orchestration platform common design pattern at spotify embedding simple kafka producer in the event delivery service also proved to be easy the mirror maker project introduced mirroring between data centers and the camus project can be used for exporting avro structured events to hourly buckets'\n",
      " 'dblog is generic changedatacapture framework that allows capturing committed changes from database in realtime and propagating those changes to downstream consumers transaction logs from databases are the source of cdc events dblog is javabased framework able to capture changes in realtime and to take dumps we use zookeeper to store state related to log and dump processing and for leader election in order to use dblog database needs to provide change log from linear history of committed changes and these conditions are fulfilled by systems like mysql postgresql mariadb etc dump processing was integrated by using sql and jdbc only requiring to implement the chunk selection and watermark update the same code is used for mysql and postgresql and can be used for other similar databases as well dblog uses an activepassive architecture one instance is active and the others are passive standbys we leverage zookeeper for leader election to determine the active instance']\n"
     ]
    }
   ],
   "source": [
    "corpus_clear = clear_text(corpus)\n",
    "print(corpus_clear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482ff6d-0afe-483b-a246-9e6f24c00688",
   "metadata": {},
   "source": [
    "### 2) Remover stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "312973ea-8f6b-442e-bec2-23c6df9ff054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def clear_stopwords(text, language):\n",
    "    filtered_text = []\n",
    "    for t in text:\n",
    "        words = [w for w in t.split() if not w in stopwords.words(language)]\n",
    "        words = ' '.join(words)\n",
    "        filtered_text.append(words)\n",
    "    return np.array(filtered_text)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db08e613-92cd-4591-8616-f8ed581aeeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['metricsdb main components cluster manager responsible assigning partitions backend servers hdfs used store mappings partitions servers backend servers responsible processing metrics small number partitions backend server keeps latest two hours data metrics memory checkpoint inmemory data every two hours durable storage blobstore coordinators responsible routing requests replica sets validates desired quorum constraints met metricsdb multizone compliant metricsdb replication factor three apache kafka producer option batch requests helped us reduce number requests queue storage'\n",
      " 'diagram describes system architecture simple overview entity cordinator access kafka platform replicas sets replica set composed two elements cluster manager access hdfs dabase backend servers access blobstore database'\n",
      " 'coordinator use kafka message service distribute transactions hadoop blob storage service'\n",
      " 'whenever event occurs spotify client sent one spotify gateways logs via syslog assigned timestamp used throughout event delivery system data needs delivered centrally located hadoop cluster data gets delivered via event delivery service written hdfs file tailer tails log files looking new events forwards event delivery service soon gets confirmation event received it’s responsibility ends event delivery service accepts events tailer transform final structured format forwards kafka brokers built restful microservice using apollo framework deployed using helios orchestration platform common design pattern spotify embedding simple kafka producer event delivery service also proved easy mirror maker project introduced mirroring data centers camus project used exporting avro structured events hourly buckets'\n",
      " 'dblog generic changedatacapture framework allows capturing committed changes database realtime propagating changes downstream consumers transaction logs databases source cdc events dblog javabased framework able capture changes realtime take dumps use zookeeper store state related log dump processing leader election order use dblog database needs provide change log linear history committed changes conditions fulfilled systems like mysql postgresql mariadb etc dump processing integrated using sql jdbc requiring implement chunk selection watermark update code used mysql postgresql used similar databases well dblog uses activepassive architecture one instance active others passive standbys leverage zookeeper leader election determine active instance']\n"
     ]
    }
   ],
   "source": [
    "corpus_filtered = clear_stopwords(corpus_clear, \"english\")\n",
    "print(corpus_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55d884-a26c-4098-bb25-c9c13a465522",
   "metadata": {},
   "source": [
    "## Usando a biblioteca sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc872573-b8a5-4c14-9d55-58f4e76dd6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(matrix, tokens):\n",
    "    doc_names = [f'text_{i+1}' for i, _ in enumerate(matrix)]\n",
    "    df = pd.DataFrame(data=matrix, index=doc_names, columns=tokens)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7a5fba4-ccdb-47aa-a2ff-9d6fe7182f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jalves/.local/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>accepts</th>\n",
       "      <th>access</th>\n",
       "      <th>active</th>\n",
       "      <th>activepassive</th>\n",
       "      <th>allows</th>\n",
       "      <th>also</th>\n",
       "      <th>apache</th>\n",
       "      <th>apollo</th>\n",
       "      <th>architecture</th>\n",
       "      <th>...</th>\n",
       "      <th>used</th>\n",
       "      <th>uses</th>\n",
       "      <th>using</th>\n",
       "      <th>validates</th>\n",
       "      <th>via</th>\n",
       "      <th>watermark</th>\n",
       "      <th>well</th>\n",
       "      <th>whenever</th>\n",
       "      <th>written</th>\n",
       "      <th>zookeeper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 208 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        able  accepts  access  active  activepassive  allows  also  apache  \\\n",
       "text_1     0        0       0       0              0       0     0       1   \n",
       "text_2     0        0       3       0              0       0     0       0   \n",
       "text_3     0        0       0       0              0       0     0       0   \n",
       "text_4     0        1       0       0              0       0     1       0   \n",
       "text_5     1        0       0       2              1       1     0       0   \n",
       "\n",
       "        apollo  architecture  ...  used  uses  using  validates  via  \\\n",
       "text_1       0             0  ...     1     0      0          1    0   \n",
       "text_2       0             1  ...     0     0      0          0    0   \n",
       "text_3       0             0  ...     0     0      0          0    0   \n",
       "text_4       1             0  ...     2     0      2          0    2   \n",
       "text_5       0             1  ...     2     1      1          0    0   \n",
       "\n",
       "        watermark  well  whenever  written  zookeeper  \n",
       "text_1          0     0         0        0          0  \n",
       "text_2          0     0         0        0          0  \n",
       "text_3          0     0         0        0          0  \n",
       "text_4          0     0         1        1          0  \n",
       "text_5          1     1         0        0          2  \n",
       "\n",
       "[5 rows x 208 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorize = CountVectorizer()\n",
    "vector_matrix = vectorize.fit_transform(corpus_filtered)\n",
    "\n",
    "tokens = vectorize.get_feature_names()\n",
    "create_dataframe(vector_matrix.toarray(),tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f27a489-3352-4e42-98fa-8e02a3e43dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diag_1</th>\n",
       "      <th>resp_1</th>\n",
       "      <th>resp_2</th>\n",
       "      <th>Diag_2</th>\n",
       "      <th>Diag_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.233862</td>\n",
       "      <td>0.075641</td>\n",
       "      <td>0.079498</td>\n",
       "      <td>0.036628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_2</th>\n",
       "      <td>0.233862</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.047565</td>\n",
       "      <td>0.080754</td>\n",
       "      <td>0.041459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_3</th>\n",
       "      <td>0.075641</td>\n",
       "      <td>0.047565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.205222</td>\n",
       "      <td>0.044699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_4</th>\n",
       "      <td>0.079498</td>\n",
       "      <td>0.080754</td>\n",
       "      <td>0.205222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.086729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_5</th>\n",
       "      <td>0.036628</td>\n",
       "      <td>0.041459</td>\n",
       "      <td>0.044699</td>\n",
       "      <td>0.086729</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Diag_1    resp_1    resp_2    Diag_2    Diag_3\n",
       "text_1  1.000000  0.233862  0.075641  0.079498  0.036628\n",
       "text_2  0.233862  1.000000  0.047565  0.080754  0.041459\n",
       "text_3  0.075641  0.047565  1.000000  0.205222  0.044699\n",
       "text_4  0.079498  0.080754  0.205222  1.000000  0.086729\n",
       "text_5  0.036628  0.041459  0.044699  0.086729  1.000000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
    "create_dataframe(cosine_similarity_matrix,['Diag_1','resp_1','resp_2', 'Diag_2', 'Diag_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d9a53f-97a6-4426-aedc-7bbbd7b27c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
