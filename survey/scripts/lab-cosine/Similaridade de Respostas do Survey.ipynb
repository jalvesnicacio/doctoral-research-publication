{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.array(['O Coordinator vai enviar as métricas para o Kafka. O Kafka possui 3 réplicas, cada uma delas contendo 1 Cluster manager e um Backend Server . O Cluster manager vai trabalhar com os HDFS enquanto o Backend vai alimentar o Blobstore.',\n",
    "          'MetricsDB tem 3 componentes principais. O Cluster Manager é responsável por atribuir partições a servidores backend. HDFS é usado para armazenar mapeamentos de partições para servidores. Os servidores de back-end são responsáveis ​​pelo processamento de métricas para um pequeno número de partições. Cada servidor de back-end mantém as últimas duas horas de dados para todas as métricas na memória e verifica os dados na memória a cada duas horas para armazenamento durável, Blobstore. Os coordenadores são responsáveis ​​por rotear solicitações para todos os conjuntos de réplicas e valida que as restrições de quorum desejadas sejam atendidas. MetricsDB é compatível com várias zonas. MetricsDB tem um fator de replicação de três. O produtor Apache Kafka tem uma opção para solicitações em lote, o que nos ajudou a reduzir o número de solicitações para a fila e armazenamento.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Aplicar lowercase e remover caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    pattern = \"[{}]\".format(string.punctuation)\n",
    "    text = [word.lower() for word in text]\n",
    "    text = [[re.sub(pattern, \"\", word) for word in words.split()] for words in text]\n",
    "    text = [[word for word in words if len(word)>1] for words in text]    \n",
    "    text = [' '.join(words) for words in text]\n",
    "    return np.array(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coordinator vai enviar as métricas para kafka kafka possui réplicas cada uma delas contendo cluster manager um backend server cluster manager vai trabalhar com os hdfs enquanto backend vai alimentar blobstore'\n",
      " 'metricsdb tem componentes principais cluster manager responsável por atribuir partições servidores backend hdfs usado para armazenar mapeamentos de partições para servidores os servidores de backend são responsáveis \\u200b\\u200bpelo processamento de métricas para um pequeno número de partições cada servidor de backend mantém as últimas duas horas de dados para todas as métricas na memória verifica os dados na memória cada duas horas para armazenamento durável blobstore os coordenadores são responsáveis \\u200b\\u200bpor rotear solicitações para todos os conjuntos de réplicas valida que as restrições de quorum desejadas sejam atendidas metricsdb compatível com várias zonas metricsdb tem um fator de replicação de três produtor apache kafka tem uma opção para solicitações em lote que nos ajudou reduzir número de solicitações para fila armazenamento']\n"
     ]
    }
   ],
   "source": [
    "corpus_clear = clear_text(corpus)\n",
    "print(corpus_clear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Separar em array de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_all(text):\n",
    "    text_set = set()\n",
    "    for w in [words.split() for words in text]:\n",
    "        text_set.update(w)\n",
    "    return np.array(list(text_set))\n",
    "\n",
    "vocabulary = text_all(corpus_clear)\n",
    "#vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Remover stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "filtered_words = [t for t in vocabulary if not t in stopwords.words(\"portuguese\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cada',\n",
       " 'opção',\n",
       " 'armazenar',\n",
       " 'todas',\n",
       " 'manager',\n",
       " 'componentes',\n",
       " 'contendo',\n",
       " 'cluster',\n",
       " 'últimas',\n",
       " 'várias',\n",
       " 'apache',\n",
       " 'quorum',\n",
       " 'ajudou',\n",
       " 'fator',\n",
       " 'kafka',\n",
       " 'dados',\n",
       " 'coordenadores',\n",
       " 'restrições',\n",
       " 'processamento',\n",
       " 'valida',\n",
       " 'produtor',\n",
       " 'durável',\n",
       " 'coordinator',\n",
       " 'usado',\n",
       " 'pequeno',\n",
       " 'atendidas',\n",
       " 'hdfs',\n",
       " 'responsável',\n",
       " 'zonas',\n",
       " 'server',\n",
       " 'réplicas',\n",
       " 'servidores',\n",
       " 'responsáveis',\n",
       " 'reduzir',\n",
       " 'memória',\n",
       " 'atribuir',\n",
       " 'possui',\n",
       " 'duas',\n",
       " 'três',\n",
       " 'enviar',\n",
       " 'blobstore',\n",
       " 'backend',\n",
       " 'enquanto',\n",
       " 'partições',\n",
       " 'trabalhar',\n",
       " '\\u200b\\u200bpelo',\n",
       " 'verifica',\n",
       " 'solicitações',\n",
       " 'vai',\n",
       " 'compatível',\n",
       " 'todos',\n",
       " 'mapeamentos',\n",
       " '\\u200b\\u200bpor',\n",
       " 'conjuntos',\n",
       " 'métricas',\n",
       " 'número',\n",
       " 'lote',\n",
       " 'rotear',\n",
       " 'mantém',\n",
       " 'horas',\n",
       " 'alimentar',\n",
       " 'desejadas',\n",
       " 'replicação',\n",
       " 'servidor',\n",
       " 'armazenamento',\n",
       " 'fila',\n",
       " 'principais',\n",
       " 'metricsdb']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(\" \".join(filtered_words))\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform(text, words=filtered_words):\n",
    "    return [int(word in text.split()) for word in words]\n",
    "\n",
    "features = np.array(list(map(fit_transform, corpus_clear)))\n",
    "#print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando similaridade do cosseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v, w):\n",
    "    return np.dot(v, w)/np.sqrt(np.dot(v, v)*np.dot(w, w))    \n",
    "    #return np.dot(v, w)/(np.linalg.norm(v)*np.linalg.norm(w)) # usando distância linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_similar(id_text, features=features, text=corpus, n_text=3):\n",
    "    simillarity = [[cosine_similarity(features[id_text], feature), int(i)] for i, feature in enumerate(features)]\n",
    "    simillarity = np.array(sorted(simillarity, key=lambda sim: sim[0], reverse=True))    \n",
    "    return [[text[y], simillarity[x, 0]] for x, y in enumerate(np.int0(simillarity[1:,1]), 1)][:n_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto analisado ->  MetricsDB tem 3 componentes principais. O Cluster Manager é responsável por atribuir partições a servidores backend. HDFS é usado para armazenar mapeamentos de partições para servidores. Os servidores de back-end são responsáveis ​​pelo processamento de métricas para um pequeno número de partições. Cada servidor de back-end mantém as últimas duas horas de dados para todas as métricas na memória e verifica os dados na memória a cada duas horas para armazenamento durável, Blobstore. Os coordenadores são responsáveis ​​por rotear solicitações para todos os conjuntos de réplicas e valida que as restrições de quorum desejadas sejam atendidas. MetricsDB é compatível com várias zonas. MetricsDB tem um fator de replicação de três. O produtor Apache Kafka tem uma opção para solicitações em lote, o que nos ajudou a reduzir o número de solicitações para a fila e armazenamento. \n",
      "\n",
      "Texto: O Coordinator vai enviar as métricas para o Kafka. O Kafka possui 3 réplicas, cada uma delas contendo 1 Cluster manager e um Backend Server . O Cluster manager vai trabalhar com os HDFS enquanto o Backend vai alimentar o Blobstore. | Similaridade: 0.27617\n"
     ]
    }
   ],
   "source": [
    "print('Texto analisado -> ',corpus[1], '\\n')\n",
    "for t, s in text_similar(1):\n",
    "    print('Texto: {} | Similaridade: {}'.format(t, round(s, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando a biblioteca sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(matrix, tokens):\n",
    "    doc_names = [f'text_{i+1}' for i, _ in enumerate(matrix)]\n",
    "    df = pd.DataFrame(data=matrix, index=doc_names, columns=tokens)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ajudou</th>\n",
       "      <th>alimentar</th>\n",
       "      <th>apache</th>\n",
       "      <th>armazenamento</th>\n",
       "      <th>armazenar</th>\n",
       "      <th>as</th>\n",
       "      <th>atendidas</th>\n",
       "      <th>atribuir</th>\n",
       "      <th>backend</th>\n",
       "      <th>blobstore</th>\n",
       "      <th>...</th>\n",
       "      <th>três</th>\n",
       "      <th>um</th>\n",
       "      <th>uma</th>\n",
       "      <th>usado</th>\n",
       "      <th>vai</th>\n",
       "      <th>valida</th>\n",
       "      <th>verifica</th>\n",
       "      <th>várias</th>\n",
       "      <th>zonas</th>\n",
       "      <th>últimas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text_1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ajudou  alimentar  apache  armazenamento  armazenar  as  atendidas  \\\n",
       "text_1       0          1       0              0          0   1          0   \n",
       "text_2       1          0       1              2          1   3          1   \n",
       "\n",
       "        atribuir  backend  blobstore  ...  três  um  uma  usado  vai  valida  \\\n",
       "text_1         0        2          1  ...     0   1    1      0    3       0   \n",
       "text_2         1        3          1  ...     1   2    1      1    0       1   \n",
       "\n",
       "        verifica  várias  zonas  últimas  \n",
       "text_1         0       0      0        0  \n",
       "text_2         1       1      1        1  \n",
       "\n",
       "[2 rows x 83 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "#features = vectorizer.fit_transform(corpus_clear).todense()\n",
    "features = vectorizer.fit_transform(corpus_clear)\n",
    "tokens = vectorizer.get_feature_names()\n",
    "\n",
    "features.toarray()\n",
    "create_dataframe(features.toarray(),tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_1</th>\n",
       "      <th>doc_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.193925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_2</th>\n",
       "      <td>0.193925</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           doc_1     doc_2\n",
       "text_1  1.000000  0.193925\n",
       "text_2  0.193925  1.000000"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer()\n",
    "vector_matrix = Tfidf_vect.fit_transform(corpus_clear, analyzer='word', stop_words='english')\n",
    "tokens2 = Tfidf_vect.get_feature_names()\n",
    "create_dataframe(vector_matrix.toarray(),tokens2)\n",
    "\n",
    "cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
    "create_dataframe(cosine_similarity_matrix,['doc_1','doc_2'])\n",
    "\n",
    "#tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_similar_sklearn(id_text, features=features, text=corpus, n_text=3):\n",
    "\n",
    "    simillarity = [[cosine_similarity(features[id_text], feature)[0,0], int(i)] for i, feature in enumerate(features)]\n",
    "    simillarity = np.array(sorted(simillarity, key=lambda sim: sim[0], reverse=True))    \n",
    "    return [[text[y], simillarity[x, 0]] for x, y in enumerate(np.int0(simillarity[1:,1]), 1)][:n_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto analisado ->  MetricsDB tem 3 componentes principais. O Cluster Manager é responsável por atribuir partições a servidores backend. HDFS é usado para armazenar mapeamentos de partições para servidores. Os servidores de back-end são responsáveis ​​pelo processamento de métricas para um pequeno número de partições. Cada servidor de back-end mantém as últimas duas horas de dados para todas as métricas na memória e verifica os dados na memória a cada duas horas para armazenamento durável, Blobstore. Os coordenadores são responsáveis ​​por rotear solicitações para todos os conjuntos de réplicas e valida que as restrições de quorum desejadas sejam atendidas. MetricsDB é compatível com várias zonas. MetricsDB tem um fator de replicação de três. O produtor Apache Kafka tem uma opção para solicitações em lote, o que nos ajudou a reduzir o número de solicitações para a fila e armazenamento. \n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-23c5f0c383d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Texto analisado -> '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_similar_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Texto: {} | Similaridade: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-17e58b02a355>\u001b[0m in \u001b[0;36mtext_similar_sklearn\u001b[0;34m(id_text, features, text, n_text)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msimillarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msimillarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimillarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimillarity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimillarity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-128-17e58b02a355>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msimillarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msimillarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimillarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimillarity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimillarity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "print('Texto analisado -> ',corpus[1], '\\n')\n",
    "for t, s in text_similar_sklearn(1):\n",
    "    print('Texto: {} | Similaridade: {}'.format(t, round(s, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
